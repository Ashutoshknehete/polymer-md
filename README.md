**polymerMD package**

This Python package is divided into three sections, namely structure, simtools, and analysis, with each containing a list of Python scripts written in an object-oriented fashion. 

**structure**

  **systemspec.py:** This code implements an object-oriented framework for constructing and describing molecular systems composed of monatomic and polymeric species. At the lowest level, MonomerSpec and BlockSpec define the chemical identity, length, and particle types of monomers and polymer blocks, while the abstract Species base class enforces a common interface (label, length, particletypes) for all molecular species. Concrete species include MonatomicMoleculeSpec for single-bead monoatomic molecules, LinearPolymerSpec for block copolymers with specified sequential connectivity, and BranchedPolymerSpec for complex architectures such as star, grafted, mikto-arm, and custom branched polymers. These polymer classes automatically generate particle types, bonds, bond types, angles, and angle types from the specified topology, explicitly handling junction nodes, free ends, and directionality of connectivity. At the system level, Component represents a population of identical molecules, Box defines the simulation domain, and System aggregates all components into a complete molecular system. The System class provides utilities to compute global particle counts and types, molecule- and block-level particle indices, bond and angle lists (both globally and per molecule), junction identification, and adjacency (bond) graphs, while maintaining consistent particle ordering for both linear and branched chains. 

  **systemgen.py:** This module provides utilities to generate initial particle coordinates and simulation snapshots for molecular systems defined using the system specification framework described above. It implements Monte Carlo–based random walk algorithms to build polymer conformations at the block level (mc_chain_walk) and specialized connection routines for linear polymer chains (connect_chains_linear), star polymers (connect_chains_star), and grafted polymers (connect_chains_graft), ensuring minimal backfolding of chains through geometric acceptance criteria. High-level walker functions (walk_linearPolymer, walk_starPolymer, walk_graftPolymer) assemble full polymer coordinates from their block specifications, while walkComponent handles repeated instances of a given species. At the system level, the code places molecules randomly within the simulation box or predefined spatial regions using placeComponent, applies periodic wrapping with wrap_coords, and assembles full-system coordinate arrays via systemCoordsRandom or systemCoordsBoxRegions. Helper functions (getParticleTypes, getBondTypes, getAngleTypes) extract particle, bond, and angle types and their corresponding indices directly from the system specification. Finally, build_snapshot integrates all structural information: box dimensions, particle positions, types, bonds, and angles into a HOOMD-compatible GSD snapshot, providing a complete, ready-to-simulate representation of the molecular system.
simtools

  **simroutines.py:** This code is a modular collection of functions for managing, running, and logging molecular dynamics simulations of polymer systems interacting with LJ and FENE potentials in HOOMD-blue. The initial setup functions like setup_LJ_FENE establish the simulation environment by applying force fields (LJ and FENE), integrators, and logging utilities. They also handle optional outputs such as trajectories, snapshots, and detailed logs, making them foundational to all subsequent routines. run_GAUSSIAN_FENE provides a specialized integration routine for overlap removal using a Gaussian pair potential with ramped energetic pre-factors. Functions like remove_overlaps and relax_overlaps_AB focus on preparing physically reasonable starting configurations. remove_overlaps uses the soft Gaussian FENE potentials to resolve particle overlaps, while relax_overlaps_AB use the NVE ensemble and a capped displacement integrator with appropriate interactions to relax the configurations. These functions are critical for preventing numerical instabilities before equilibration. Equilibration functions such as equilibrate_AB, equilibrate_npt, simulate_npat instantiate simulations through Langevin or NPT/NPAT dynamics. They differ in the type of ensemble and integrator used and are central for preparing the system for meaningful production runs. The production function executes equilibrium simulation under controlled temperature and optionally zeroes net momentum to remove bulk drift. The framework also includes robust logging and analysis utilities. basic_logger and add_table_log track thermodynamic and global simulation properties. The functions add_filtered_thermo and run_filtered_thermo enable spatially resolved thermodynamic monitoring along a chosen axis, outputting both GSD logs and edge data. Snapshot and trajectory writers (add_write_state and add_write_trajectory) allow capturing system states at specific times or periodically. 

**analysis**

  **trajtools.py:** This module provides a comprehensive suite of analysis functions. The functions are designed to extract structural, spatial, and thermodynamic information from simulation frames or trajectories. At the core, getBondedClusters identifies bonded molecular clusters using neighbor lists, enabling further calculations of internal distances and conformational properties. Density functions (density_system, density_1D_monomers, density_1D_species, density_1D_lowest) quantify the number density or volume fraction along a specific axis or in full 3D, allowing characterization of phase separation or spatial distributions of monomers and species. Several functions target polymer conformational analysis: internaldistances_all and internaldistances_species compute mean-squared internal distances within homopolymer or copolymer chains species; radius_gyration, molecule_stretching, and molecule_stretching_monomer measure overall and directional radii of gyration, providing insights into copolymer shape, and anisometry. For block copolymers, lineardistancesfromjunctions and endToEndVectors compute distance statistics from junction points and end-to-end vectors, useful for understanding chain orientation and stretching behavior. Spatial organization of junctions on interfaces is addressed with junction_RDF, junction_locations, junction_RDF_accumulate, and junction_density_smeared functions, which compute radial distribution functions, positions, or smoothed density fields of junctions across the system or accumulated over multiple frames. Volume fraction fields in 1D or 3D are computed via volfrac_fields and volfrac_fields_species, supporting the computation of overlap integrals (overlap_integral, overlap_integral_species, overlap_integral_species_normalized) to quantify mixing between homopolymer and copolymer species. Finally, interfacial properties are analyzed with interfacial_tension_IK and interfacial_tension_global, which estimate interfacial tension from local or global pressure tensors. Utility functions like ensemble_average_log allow averaging of logged simulation data across frames. 

  **statistics.py:** This module provides tools for analyzing time-correlated simulation data and extracting statistically significant data. The core function, estimate_autocorrelation_time, uses block averaging and linear regression to estimate the autocorrelation time of a time series or multi-dimensional property, capturing how quickly the system decorrelates. Building on this, num_independent_samples computes the effective number of independent samples by dividing the total number of samples by twice the autocorrelation time, providing a corrected measure of sample size for statistical analysis. The estimator_variance function uses this information to calculate the variance of the mean for finite-length trajectories, accounting for correlations and yielding a robust estimate of uncertainty. Finally, get_independent_samples produces a reduced set of independent samples by averaging over blocks corresponding to the autocorrelation time, enabling subsequent analyses to be performed on statistically independent data. 

**Signac Module**

The Signac framework is used to manage project-related data with a well-defined indexable storage layout for data and metadata. This approach ensures reproducibility, enables efficient post-processing and analysis, and provides collective access to all project data. In Signac, simulation inputs and outputs are organized using a consistent directory hierarchy that is automatically generated from parameter metadata. An exhaustive  documentation for Signac is available. We recommend starting with the ideal gas example project.

A Signac project consists of a collection of jobs, where each job corresponds to a unique statepoint representing an irreducible set of system parameters. The statepoint uniquely defines the physical system in a JSON-like file, and remains unchangeable. Each job also contains a job document, a JSON-like file used to store small or derived data such as computed properties or indicators of completed simulation stages. Jobs may be grouped into aggregates based on logical criteria to facilitate collective analysis.

The dataspace is constructed by initializing a project and creating jobs over the desired parameter ranges. Signac provides Python interfaces and commands like signac schema and signac view for navigating, querying, and modifying the dataspace, allowing efficient inspection of the project structure and metadata schema.

Workflow execution is handled by Signac-flow, which enables the definition of logical workflows over the dataspace. Computational tasks are implemented as operations, and their execution is governed by pre-conditions and post-conditions. Pre-conditions ensure that operations are executed only when all required inputs are available, while post-conditions prevent redundant execution by marking completed steps. Related operations can be grouped to enforce sequential execution of dependent tasks. This conditional execution model allows Signac-flow to automatically determine the correct execution order across all jobs in the project.

Signac-flow also provides tools to track the execution status of each job relative to the defined workflow and supports automated submission of operations to high-performance computing clusters. Upon completion of simulations and analyses, job-level data and metadata can be exported to tabular formats such as pandas DataFrames and stored as serialized pickle files or CSV files for further analysis and visualization.

**Project workspace **

Our simulation workflow uses Signac to organize simulations into well-defined state points, automate job execution, manage metadata, and do ensemble averaging over statistically independent replicas. Each project is initialized by defining a multidimensional parameter space over which simulations are performed. Each grid point in this parameter space corresponds to a state point, representing a unique combination of system parameters such as polymer composition and architecture, interaction parameters, and thermodynamic conditions. For each state point, multiple statistically independent replicas are generated by varying the copolymer initialization, ensuring statistically significant results.

Project initialization is performed using the Python script init_project.py, which creates the Signac workspace via the signac.init_project() command. Individual jobs are generated by defining a state point sp and initializing it using project.open_job(sp).init(), which stores each job in a unique hashed subdirectory. State-point-specific metadata for each job is recorded in the corresponding job_statepoint.json file within that directory. Additional state points can be added at any stage without restructuring the project, allowing scalable exploration of the parameter space by reloading the project using signac.get_project().

  **Simulation: jobs**

The file project_job.py serves as the central workflow controller for all simulations and analyses performed within the Signac workspace across large parameter spaces. It defines the complete simulation pipeline as a collection of operations that act on independent jobs (state points), with execution and dependency management handled by the Signac-Flow framework. The workflow is organized using user-defined helper functions and classes, labels, operation groups, and operations. 

    **Helper functions and classes**
Helper functions encapsulate recurring simulation tasks in a concise, modular, and object-oriented manner, and are invoked by individual operations. For example, the build_system() helper function constructs the initial configuration and internally calls specialized routines such as build_system_spec_graft() and compute_box_dimensions(). Interaction potentials between coarse-grained bead types are defined using the get_potential() function, while setup_nvt() and setup_npt() functions configure the molecular dynamics integrators and thermodynamic ensembles used in the simulations. Further, distinct stages of the simulation protocol are implemented through dedicated helper functions. The _relax() function removes unfavorable overlaps in the initially generated configuration, and the _equilibrate() routine equilibrates the relaxed configuration. The _cool() function applies controlled cooling below the glass transition temperature using a temperature ramp, and _elongate() performs uniaxial deformation at a prescribed strain rate to probe the mechanical response of the system. These routines extensively leverage functionalities from the polymer-md package. 

    **Simulation and analysis labels**
    
Progress through the workflow is tracked using labels, which define logical pre- and post-conditions for each operation. Labels are typically implemented by checking for the presence of specific output files or entries in the job document. Simulation labels such as initialized, relaxed, equilibrated, cooled, and stretched indicate whether key stages of the simulation pipeline have been successfully completed, while various analysis labels track the completion of the corresponding post-processing tasks. These labels allow Signac to automatically determine job eligibility for a given operation, prevent unnecessarily redundant computation, and track progress across large ensembles of statistically independent simulations. Importantly, the workflow is fault-tolerant and restartable: if an operation fails or produces unphysical results for a given state point, that job can be selectively reset and resumed from the last valid stage without repeating the entire pipeline. Labels may also be manually modified to force re-execution of specific operations across all replicas or state points when methodological updates are introduced.
Operations and operation groups

Operations in the Signac workflow are organized into logical operation groups, most notably simulation and analysis. Simulation tasks are grouped under a simulation operation group and proceed through sequential stages of system generation, relaxation, equilibration in the melt state, cooling below the glass transition temperature, and uniaxial elongation. Each operation explicitly declares its dependencies through pre-conditions and its successful completion through post-conditions defined by labels. This structure enforces a well-defined execution order along with independent re-running of individual operations.

Analysis tasks are similarly grouped under an analysis operation group and compute structural, conformational, and mechanical descriptors from the equilibrated snapshot or from the mechanical deformation trajectory. These operations are triggered only after the completion of required simulation stages, as determined by their label-based pre-conditions. Analysis operations include the calculation of interfacial tension, macroscopic pressure components, mean-squared internal distances, copolymer volume fraction profiles, stress-strain, lowest-density-strain, and copolymer-conformation-strain response, end-to-end vector statistics, species overlap integrals, strain-at-break, failure location identification, copolymer failure location, and topological entanglement analysis using the Z1+ algorithm. Each analysis task produces standardized output files (e.g., serialized data objects) that are subsequently tracked by corresponding post-condition labels.

    **Execution**

Execution of the workflow is automated through Signac’s command-line interface. The status command reports the number of eligible and completed jobs for each operation, while the run or submit commands execute operations either interactively or in batch mode on high-performance computing systems using SLURM. This framework enables scalable, reproducible simulations spanning hundreds to thousands of independent polymer systems, while maintaining a single, unified codebase for setup, simulation, and analysis.

  **Analysis: aggregation**

Once all replicas for a given state point are completed, an aggregation step combines statistically independent trajectories into ensemble-averaged datasets. This aggregation process excludes replicas that fail equilibration or quality criteria and produces averaged structural and thermodynamic observables suitable for direct comparison across systems. Aggregated data are stored in a separate hashed sub-directory structure that mirrors Signac’s organizational principles while representing system-level rather than replica-level information. Following aggregation, averaged data are exported into tabular formats to facilitate further analysis and visualization. Dedicated analysis scripts transform these datasets into the final quantities reported in the manuscripts, including spatial profiles, correlation functions, and interfacial metrics. 

The project_aggregate.py script implements this workflow by providing a structured framework for combining and analyzing simulation data from multiple replicas of a given statepoint. It begins with helper functions to define polymer architectures (graft, miktoarm, linear) using functionalities from the systemspec.py code, specifying monomers, polymer blocks, and copolymer structures. Central to the aggregation step are the replicaAggregate and openpath classes. replicaAggregate manages filesystem context, organizes jobs by statepoint, distinguishes complete and incomplete replicas, ensures all jobs belong to the same statepoint, and provides convenient access to documents (doc) and statepoints (sp). The openpath class allows safe directory context management during aggregation operations. The replica_aggregator() function and replica_key() define how jobs are grouped for aggregation within a FlowProject. Pre- and post-conditions enforce that aggregation operations run only when all replicas meet specified criteria, such as being stretched, cooled, or containing required output files. Labelling functions check whether specific aggregate-level outputs already exist, avoiding redundant calculations. The make_data_dirs utility ensures that necessary directories are created for storing aggregate outputs. Operations are grouped under an aggregate_group within the StrainAgg FlowProject. Each operation is decorated with pre- and post-conditions and an aggregator. These include computing junction densities, junction density uniformity, interfacial tension, mechanical properties (stress-strain curves, peak stress, toughness, and modulus), stress tensor components, density profiles (lowest density and 1D spatial distributions), copolymer conformational analysis, failure statistics, normalized overlaps between polymer species, and topological entanglements. The results are stored in the aggregate document or the hashed subfolder for subsequent analysis and visualization.



